{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Classfication \n",
    "A For CZ4041 Machine Learning Assignment from PT3 in AY2018/2019 Semester 2.\n",
    "The group members are:\n",
    "- LIU Yiqing\n",
    "- LUO Bingyi\n",
    "- TENG He Xu\n",
    "- WANG Jia\n",
    "- YI Zhiyue\n",
    "- ZHAO Ziru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries and Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "IMAGE_PATH = 'data/images/'\n",
    "LABEL_PATH = 'data/'\n",
    "TRAIN_FILE_NAME = 'train.csv'\n",
    "TEST_FILE_NAME = 'test.csv'\n",
    "COMMON_HEIGHT_WIDTH = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images from the given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    images = os.listdir(path)\n",
    "    loaded_images = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        loaded_images.append(path + images[i])\n",
    "    return loaded_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize the images to a uniformed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_resize(file_name, hw):\n",
    "    image = cv2.imread(file_name, 0)\n",
    "    new_img = cv2.resize(image, (int(hw), int(hw)))\n",
    "    return np.reshape(new_img, (int(hw), int(hw), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load labels from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(file_path):\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        # Skip the first line\n",
    "        next(csv_reader)\n",
    "        # Remove empty lines\n",
    "        lines = list(line for line in csv_reader if line)\n",
    "        for line in lines:\n",
    "            label = {}\n",
    "            label['id'] = int(line[0])\n",
    "            label['species'] = line[1]\n",
    "            labels.append(label)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images_and_labels(images, labels):\n",
    "    id = []\n",
    "    x = []\n",
    "    y = []\n",
    "    for image in images:\n",
    "        for label in labels:\n",
    "            if image['id'] == label['id']:\n",
    "                x.append(image['image'])\n",
    "                y.append(label['species'])\n",
    "                id.append( image['id'])\n",
    "    \n",
    "    return {\n",
    "        'id': np.array(id),\n",
    "        'x': np.array(x),\n",
    "        'y': np.array(y)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructDicitionary(labels):\n",
    "    uniqueLabels = list(set(list(map(lambda x: x[\"species\"], labels))))\n",
    "    dictionary = []\n",
    "    for i in range(len(uniqueLabels)):\n",
    "        dictionary.append({\n",
    "            \"number\": i,\n",
    "            \"text\": uniqueLabels[i]\n",
    "        })\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(dictionary, text):\n",
    "    for i in range(len(dictionary)):\n",
    "        if dictionary[i][\"text\"] == text:\n",
    "            return dictionary[i][\"number\"]\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(dictionary, number):\n",
    "    for i in range(len(dictionary)):\n",
    "        if dictionary[i][\"number\"] == number:\n",
    "            return dictionary[i][\"text\"]\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images(IMAGE_PATH)\n",
    "train_labels = load_labels(LABEL_PATH + TRAIN_FILE_NAME)\n",
    "resized_images = []\n",
    "\n",
    "dictionary = constructDicitionary(train_labels)\n",
    "\n",
    "for i in range(len(images)):\n",
    "    resized_image = image_resize(images[i], COMMON_HEIGHT_WIDTH)\n",
    "    record = {}\n",
    "    record['id'] = int(images[i].split('.')[0].split('/')[2])\n",
    "    record['image'] = resized_image\n",
    "    resized_images.append(record)\n",
    "    \n",
    "## for img in resized_images:\n",
    "##     plt.imshow(img)\n",
    "##     plt.show()\n",
    "\n",
    "for j in range(len(train_labels)):\n",
    "    train_labels[j][\"species\"] = encode(dictionary, train_labels[j][\"species\"])\n",
    "\n",
    "train_data = combine_images_and_labels(resized_images, train_labels)\n",
    "train_x = train_data['x']\n",
    "train_y = train_data['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c10fa84b37dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_y' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_49 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 22, 22, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 9, 9, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 99)                6435      \n",
      "=================================================================\n",
      "Total params: 100,099\n",
      "Trainable params: 100,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lane0\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(50, 50, 1...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\lane0\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  import sys\n",
      "c:\\users\\lane0\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(50, 50, 1)))       \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(99, activation=tf.nn.softmax))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 50, 50, 1)\n",
      "(990,)\n",
      "Epoch 1/30\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 3.3529 - acc: 0.1667\n",
      "Epoch 2/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 3.1069 - acc: 0.1899\n",
      "Epoch 3/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 3.0552 - acc: 0.2071\n",
      "Epoch 4/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.8450 - acc: 0.2414\n",
      "Epoch 5/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.7908 - acc: 0.2545\n",
      "Epoch 6/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.7730 - acc: 0.2434\n",
      "Epoch 7/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.6266 - acc: 0.2879\n",
      "Epoch 8/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.4921 - acc: 0.3071\n",
      "Epoch 9/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.4403 - acc: 0.3081\n",
      "Epoch 10/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.3086 - acc: 0.3253\n",
      "Epoch 11/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.3005 - acc: 0.3566\n",
      "Epoch 12/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.3149 - acc: 0.3313\n",
      "Epoch 13/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.1433 - acc: 0.3677\n",
      "Epoch 14/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.0607 - acc: 0.3939\n",
      "Epoch 15/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 2.0526 - acc: 0.4020\n",
      "Epoch 16/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.9975 - acc: 0.4091\n",
      "Epoch 17/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.9228 - acc: 0.4192\n",
      "Epoch 18/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.9501 - acc: 0.4121\n",
      "Epoch 19/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.9112 - acc: 0.4081\n",
      "Epoch 20/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.8152 - acc: 0.4616\n",
      "Epoch 21/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.7938 - acc: 0.4444\n",
      "Epoch 22/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.8117 - acc: 0.4394A: 0s - loss: 1.7786 -\n",
      "Epoch 23/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.7511 - acc: 0.4596\n",
      "Epoch 24/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.7232 - acc: 0.4697\n",
      "Epoch 25/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.7161 - acc: 0.4737\n",
      "Epoch 26/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.7137 - acc: 0.4788\n",
      "Epoch 27/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.6743 - acc: 0.4818\n",
      "Epoch 28/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.6035 - acc: 0.4949A: 0s - loss: 1.5856 - acc:\n",
      "Epoch 29/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.5995 - acc: 0.5121\n",
      "Epoch 30/30\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 1.5676 - acc: 0.5040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2025470ac50>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(np.shape(train_x))\n",
    "print(np.shape(train_y))\n",
    "model.fit(train_x, train_y, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = load_labels(LABEL_PATH + TEST_FILE_NAME)\n",
    "\n",
    "for k in range(len(test_labels)):\n",
    "    test_labels[k][\"species\"] = encode(dictionary, test_labels[k][\"species\"])\n",
    "\n",
    "test_data = combine_images_and_labels(resized_images, test_labels)\n",
    "\n",
    "test_x = test_data['x']\n",
    "test_y = test_data['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(test_x)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "with open('submission.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id']\n",
    "    \n",
    "    label_fieldslist = list(set(list(map(lambda x: x[\"species\"], train_labels))))\n",
    "    for i in range(len(label_fieldslist)):\n",
    "        fieldnames.append(decode(dictionary, label_fieldslist[i]))\n",
    "        \n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for n in range(len(output)):\n",
    "        #print(np.argmax(output[n]))\n",
    "        row = {}\n",
    "        for field in fieldnames:\n",
    "            species = decode(dictionary, np.argmax(output[n]))\n",
    "            row[\"id\"] = test_data['id'][n]\n",
    "            row[species] = 1\n",
    "            if field not in [\"id\", species]:\n",
    "                row[field] = 0\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
